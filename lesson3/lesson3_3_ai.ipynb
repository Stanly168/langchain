{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e43316e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ AI å›æ‡‰ï¼š\n",
      "{'error': \"model 'gemma3:1b' not found\"}\n",
      "No expected key found in response. Available keys: dict_keys(['error'])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #åƒè€ƒèªªæ˜1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"ğŸ’¬ AI å›æ‡‰ï¼š\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "#ç¯„ä¾‹è¼¸å…¥\n",
    "chat_with_ollama(\"è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522d51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤– æ¸¬è©¦æ¨¡å‹: gemma3:1b\n",
      "==================================================\n",
      "âŒ gemma3:1b å›æ‡‰ç•°å¸¸\n",
      "ğŸ” å®Œæ•´å›æ‡‰: {'error': \"model 'gemma3:1b' not found\"}\n",
      "\n",
      "ğŸ¤– æ¸¬è©¦æ¨¡å‹: gpt-oss:20b\n",
      "==================================================\n",
      "âœ… gpt-oss:20b å›æ‡‰æ­£å¸¸\n",
      "ğŸ“ å›ç­”: **Python çš„å‡½å¼ï¼ˆfunctionï¼‰** å°±åƒæ˜¯ä¸€å€‹ã€Œå°å·¥å…·ç®±ã€ï¼Œä½ å¯ä»¥æŠŠä¸€æ®µé‡è¤‡ä½¿ç”¨çš„ç¨‹å¼ç¢¼å¯«é€²å»ï¼Œä¹‹å¾Œåªè¦ã€Œå‘¼å«ã€å®ƒï¼Œå°±èƒ½ä¸€æ¬¡æ€§åŸ·è¡Œé‚£æ®µç¨‹å¼ç¢¼ã€‚\n",
      "\n",
      "### ä¸»è¦ç‰¹é»\n",
      "\n",
      "| ä»€éº¼ | ä»€éº¼æ„æ€ |\n",
      "|------|----------|\n",
      "| **å®šç¾©** | ç”¨ `def` é—œéµå­—å¯«ä¸‹ä¾†ï¼Œä¾‹å¦‚ `def my_function():` |\n",
      "| **åƒæ•¸** | ä½ å¯ä»¥æŠŠè³‡æ–™ã€Œå‚³é€²å»ã€å‡½å¼...\n"
     ]
    }
   ],
   "source": [
    "# æ¯”è¼ƒå…©å€‹æ¨¡å‹çš„å›æ‡‰\n",
    "import requests\n",
    "\n",
    "def test_model(model_name, prompt):\n",
    "    print(f\"\\nğŸ¤– æ¸¬è©¦æ¨¡å‹: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        result = response.json()\n",
    "        \n",
    "        if \"response\" in result and result[\"response\"].strip():\n",
    "            print(f\"âœ… {model_name} å›æ‡‰æ­£å¸¸\")\n",
    "            print(f\"ğŸ“ å›ç­”: {result['response'][:200]}...\")\n",
    "        else:\n",
    "            print(f\"âŒ {model_name} å›æ‡‰ç•°å¸¸\")\n",
    "            print(f\"ğŸ” å®Œæ•´å›æ‡‰: {result}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "\n",
    "# æ¸¬è©¦å…©å€‹æ¨¡å‹\n",
    "prompt = \"è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ\"\n",
    "test_model(\"gemma3:1b\", prompt)\n",
    "test_model(\"gpt-oss:20b\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabdea2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
